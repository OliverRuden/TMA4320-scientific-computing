{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test om koden er riktig implementert\n",
    "\n",
    "Her er et forslag til testfunksjoner for Ã¥ sjekke om koden er riktig implementert.\n",
    "```assert variabel``` vil gi en feilmelding med mindre variabelen ```variabel = True```. For eksempel vil ```assert a == b``` gi en feilmelding med mindre ```a``` og ```b``` er like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For eksempel:\n",
    "variable = True\n",
    "assert variable, \"You need to change 'variable' to True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import *\n",
    "from neural_network import NeuralNetwork\n",
    "from utils import onehot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from data_generators import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We choose some arbitrary values for the dimensions\n",
    "b = 1\n",
    "n_max = 7\n",
    "m = 8\n",
    "n = 5\n",
    "\n",
    "d = 10\n",
    "k = 5\n",
    "p = 15\n",
    "\n",
    "#Create an arbitrary dataset\n",
    "x = np.random.randint(0, m, (b,n))\n",
    "y = np.random.randint(0, m, (b,n_max))\n",
    "\n",
    "#initialize the layers\n",
    "feed_forward = FeedForward(d,p)\n",
    "attention = Attention(d,k)\n",
    "embed_pos = EmbedPosition(n_max,m,d)\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "\n",
    "\n",
    "#a manual forward pass\n",
    "X = onehot(x, m)\n",
    "z0 = embed_pos.forward(X)\n",
    "z1 = feed_forward.forward(z0)\n",
    "z2 = attention.forward(z1)\n",
    "z3 = un_embed.forward(z2)\n",
    "Z = softmax.forward(z3) \n",
    "\n",
    "\n",
    "\n",
    "#check the shapes\n",
    "assert X.shape == (b,m,n), f\"X.shape={X.shape}, expected {(b,m,n)}\"\n",
    "assert z0.shape == (b,d,n), f\"z0.shape={z0.shape}, expected {(b,d,n)}\"\n",
    "assert z1.shape == (b,d,n), f\"z1.shape={z1.shape}, expected {(b,d,n)}\"\n",
    "assert z2.shape == (b,d,n), f\"z2.shape={z2.shape}, expected {(b,d,n)}\"\n",
    "assert z3.shape == (b,m,n), f\"z3.shape={z3.shape}, expected {(b,m,n)}\"\n",
    "assert Z.shape == (b,m,n), f\"Z.shape={Z.shape}, expected {(b,m,n)}\"\n",
    "\n",
    "#is X one-hot?\n",
    "assert X.sum() == b*n, f\"X.sum()={X.sum()}, expected {b*n}\"\n",
    "\n",
    "\n",
    "assert np.allclose(Z.sum(axis=1), 1), f\"Z.sum(axis=1)={Z.sum(axis=1)}, expected {np.ones(b)}\"\n",
    "assert np.abs(Z.sum() - b*n) < 1e-5, f\"Z.sum()={Z.sum()}, expected {b*n}\"\n",
    "assert np.all(Z>=0), f\"Z={Z}, expected all entries to be non-negative\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1390488288412413\n",
      "0.0005463492420150697\n",
      "0.00020090857398301812\n",
      "0.00011643764429116687\n",
      "8.00390770084798e-05\n",
      "6.0180585371000766e-05\n",
      "4.7822996694314806e-05\n",
      "3.948308500066716e-05\n",
      "3.348384947646154e-05\n",
      "2.8978098215095232e-05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#test the forward pass\n",
    "n_y = n_max - 1\n",
    "x = np.random.randint(0, m, (b,n_max))\n",
    "X = onehot(x, m)\n",
    "\n",
    "#we test with a y that is shorter than the maximum length\n",
    "y = np.random.randint(0, m, (b,n_y))\n",
    "\n",
    "#initialize a neural network based on the layers above\n",
    "network = NeuralNetwork([embed_pos, feed_forward, attention, un_embed, softmax])\n",
    "#and a loss function\n",
    "loss = CrossEntropy()\n",
    "\n",
    "N = 10000\n",
    "#do a forward pass\n",
    "for i in range(N):\n",
    "    Z = network.forward(X)\n",
    "\n",
    "    #compute the loss\n",
    "    L = loss.forward(Z, y)\n",
    "    if not (i%(N//10)):\n",
    "        print(L)\n",
    "\n",
    "    #get the derivative of the loss wrt Z\n",
    "    grad_Z = loss.backward()\n",
    "\n",
    "    #and perform a backward pass\n",
    "    _ = network.backward(grad_Z)\n",
    "\n",
    "    #and and do a gradient descent step\n",
    "    _ = network.step_gd(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(neuralNetwork, objectFunction, dataSet, nIter, m, alpha = 0.01, beta_1 = 0.9, beta_2 = 0.999):\n",
    "    L = []\n",
    "    for i in range(nIter):\n",
    "        for batchNumber, x in enumerate(dataSet[0]):\n",
    "            X = onehot(x,m)\n",
    "            Z = neuralNetwork.forward(X)\n",
    "            L.append(objectFunction.forward(Z,dataSet[1][batchNumber]))\n",
    "            grad_Z = objectFunction.backward()\n",
    "            neuralNetwork.backward(grad_Z)\n",
    "            neuralNetwork.adamStep(i, batchNumber, len(dataSet[0]), alpha = alpha, beta_1 = beta_1, beta_2 = beta_2)\n",
    "    return neuralNetwork, L\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSortData(numberOfBatches, batchSize, n_max, m):\n",
    "    x = np.random.randint(0,m,(numberOfBatches,batchSize, (n_max+1)//2))\n",
    "    y = np.sort(x, axis = 2)\n",
    "    x = np.append(x,y, axis = 2)[:,:,:-1]\n",
    "    return [x,y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nBatches = 10\n",
    "# batchSize = 250\n",
    "# n_max = 9\n",
    "# m = 8\n",
    "# n_iter = 500\n",
    "# feed_forward = FeedForward(d,p)\n",
    "# attention = Attention(d,k)\n",
    "# feed_forward2 = FeedForward(d,p)\n",
    "# attention2 = Attention(d,k)\n",
    "# embed_pos = EmbedPosition(n_max,m,d)\n",
    "# un_embed = LinearLayer(d,m)\n",
    "# softmax = Softmax()\n",
    "# network = NeuralNetwork([embed_pos, feed_forward, attention, feed_forward2, attention2, un_embed, softmax])\n",
    "# loss = CrossEntropy()\n",
    "# dataSet = generateSortData(nBatches, batchSize, n_max, m)\n",
    "# network,L = training(network, loss, dataSet, n_iter, m, alpha = 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(np.arange(0,len(L),1), L)\n",
    "# plt.show()\n",
    "# print(L[-1])\n",
    "# N = 1000\n",
    "# y = np.random.randint(0,m,(N,5))\n",
    "# s = np.copy(y)\n",
    "# s = np.sort(s, axis = 1)\n",
    "# for i in range(5):\n",
    "#     z = np.argmax(network.forward(onehot(y,m)), axis = 1)\n",
    "#     y = np.append(y, z[:,-1:], axis = 1)\n",
    "# y = y[:,5:]\n",
    "# t = 0\n",
    "# for i in range(len(s)):\n",
    "#     equal = True\n",
    "#     for j in range(len(s[i])):\n",
    "#         if s[i,j] != y[i,j]:\n",
    "#             equal = False\n",
    "#     if equal:\n",
    "#         t += 1\n",
    "# print(t/N*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nHere you may add additional tests to for example:\\n\\n- Check if the ['d'] keys in the parameter dictionaries are not None, or receive something when running backward pass\\n- Check if the parameters change when you perform a gradient descent step\\n- Check if the loss decreases when you perform a gradient descent step\\n\\nThis is voluntary, but could be useful.\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here you may add additional tests to for example:\n",
    "\n",
    "- Check if the ['d'] keys in the parameter dictionaries are not None, or receive something when running backward pass\n",
    "- Check if the parameters change when you perform a gradient descent step\n",
    "- Check if the loss decreases when you perform a gradient descent step\n",
    "\n",
    "This is voluntary, but could be useful.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#check if loss is non-negative\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m L[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, expected L>=0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m grad_Z\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m Z\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_Z.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrad_Z\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mZ\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#check if onehot(y) gives zero loss\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "#check if loss is non-negative\n",
    "assert L[-1] >= 0, f\"L={L}, expected L>=0\"\n",
    "assert grad_Z.shape == Z.shape, f\"grad_Z.shape={grad_Z.shape}, expected {Z.shape}\"\n",
    "\n",
    "#check if onehot(y) gives zero loss\n",
    "Y = onehot(y, m)\n",
    "L = loss.forward(Y, y)\n",
    "assert L < 1e-5, f\"L={L}, expected L<1e-5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [15], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m CrossEntropy()\n\u001b[0;32m     20\u001b[0m dataSet \u001b[38;5;241m=\u001b[39m get_train_test_addition(n_digit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,samples_per_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m250\u001b[39m,n_batches_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, n_batches_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m network,L \u001b[38;5;241m=\u001b[39m training(network, loss, [dataSet[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_train\u001b[39m\u001b[38;5;124m'\u001b[39m],dataSet[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_train\u001b[39m\u001b[38;5;124m'\u001b[39m]], n_iter, m, alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m)\n",
      "Cell \u001b[1;32mIn [5], line 9\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m(neuralNetwork, objectFunction, dataSet, nIter, m, alpha, beta_1, beta_2)\u001b[0m\n\u001b[0;32m      7\u001b[0m         L\u001b[38;5;241m.\u001b[39mappend(objectFunction\u001b[38;5;241m.\u001b[39mforward(Z,dataSet[\u001b[38;5;241m1\u001b[39m][batchNumber]))\n\u001b[0;32m      8\u001b[0m         grad_Z \u001b[38;5;241m=\u001b[39m objectFunction\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m----> 9\u001b[0m         \u001b[43mneuralNetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_Z\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m         neuralNetwork\u001b[38;5;241m.\u001b[39madamStep(i, batchNumber, \u001b[38;5;28mlen\u001b[39m(dataSet[\u001b[38;5;241m0\u001b[39m]), alpha \u001b[38;5;241m=\u001b[39m alpha, beta_1 \u001b[38;5;241m=\u001b[39m beta_1, beta_2 \u001b[38;5;241m=\u001b[39m beta_2)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m neuralNetwork, L\n",
      "File \u001b[1;32mc:\\Users\\aasmu\\Downloads\\NTNU\\4. sem\\Vitber\\VitBerIndMat\\neural_network.py:30\u001b[0m, in \u001b[0;36mNeuralNetwork.backward\u001b[1;34m(self, grad)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#reversed yields the layers in reversed order\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m---> 30\u001b[0m     grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grad\n",
      "File \u001b[1;32mc:\\Users\\aasmu\\Downloads\\NTNU\\4. sem\\Vitber\\VitBerIndMat\\layers.py:107\u001b[0m, in \u001b[0;36mAttention.backward\u001b[1;34m(self, grad)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW_K\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkd,bdn,bno,bfo->bkf\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW_Q\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz, g_S, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz, optimize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW_Q\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkd,bdn,bon,bfo->bkf\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW_K\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz, g_S, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz, optimize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grad \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbdo,bno->bdn\u001b[39m\u001b[38;5;124m'\u001b[39m, g_OV, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mke,kd,bdn,bno->beo\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW_K\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW_Q\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz, g_S, optimize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mke,kd,bdn,bon->beo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mW_Q\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mW_K\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_S\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\aasmu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\einsumfunc.py:1371\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[0;32m   1369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m specified_out:\n\u001b[0;32m   1370\u001b[0m         kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m out\n\u001b[1;32m-> 1371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m c_einsum(\u001b[38;5;241m*\u001b[39moperands, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;66;03m# Check the kwargs to avoid a more cryptic error later, without having to\u001b[39;00m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;66;03m# repeat default values here\u001b[39;00m\n\u001b[0;32m   1375\u001b[0m valid_einsum_kwargs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcasting\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_max = 6\n",
    "m = 10\n",
    "n_iter = 20\n",
    "\n",
    "d = 30\n",
    "k = 20\n",
    "p = 40 \n",
    "\n",
    "feed_forward1 = FeedForward(d,p)\n",
    "attention1 = Attention(d,k)\n",
    "feed_forward2 = FeedForward(d,p)\n",
    "attention2 = Attention(d,k)\n",
    "feed_forward3 = FeedForward(d,p)\n",
    "attention3 = Attention(d,k)\n",
    "embed_pos = EmbedPosition(n_max,m,d)\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "network = NeuralNetwork([embed_pos, feed_forward1, attention1, feed_forward2, attention2, feed_forward3, attention3, un_embed, softmax])\n",
    "loss = CrossEntropy()\n",
    "dataSet = get_train_test_addition(n_digit=2,samples_per_batch = 250,n_batches_train = 20, n_batches_test = 4)\n",
    "network,L = training(network, loss, [dataSet['x_train'],dataSet['y_train']], n_iter, m, alpha = 0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(0,len(L),1), L)\n",
    "plt.show()\n",
    "print(L[-1])\n",
    "\n",
    "for batch_number in range(np.shape(dataSet['x_train'][0])):\n",
    "\n",
    "    t=0\n",
    "\n",
    "    for i in range(3):\n",
    "        z = np.argmax(network.forward(onehot(dataSet['x_train'][batch_number],m)), axis = 1)\n",
    "        dataSet['x_train'][batch_number] = np.append(dataSet['x_train'], z[:,-1:], axis = 1)\n",
    "\n",
    "    for x in range(np.shape(dataSet['x_train'][0])):\n",
    "\n",
    "        useful_x_values = dataSet['x_train'][batch_number][x,-3:]\n",
    "        equal = True\n",
    "\n",
    "        for digit in range(3):\n",
    "            if useful_x_values[x,digit]!=dataSet['y_train'][batch_number,x,digit]:\n",
    "                equal = False\n",
    "\n",
    "        if equal:\n",
    "            t+=1\n",
    "        \n",
    "    print(t/250*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
